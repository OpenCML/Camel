module main

import { dense, dropout, relu, softmax } from 'nn'
import opt from 'opt'

inner type Tensor = 'std::Tensor'

type Model = (x: Tensor) => Tensor

with <weights: Tensor, biases: Tensor>
inner func linear(x: Tensor): Tensor {
    // return 'std::linear'
}

with <activation: Model, weights: Tensor, biases: Tensor>
func dense(x: Tensor): Tensor {
    return x->linear<weights, biases>->activation
}

with <var weights: Tensor[], var biases: Tensor[]>
macro sync func MLP(input: int, configs: (int, Model?)[]): Model {
    return if configs.len() == 0 then {
        return (x: Tensor) => x
    } else {
        let output, act = configs.head()
        wait weights.push(Tensor::new([input, output]))
        wait biases.push(Tensor::new([output]))
        let layer = dense<act ?? relu, weights, biases>
        let rest = wait MLP<weights, biases>(output, configs.tail())
        return layer..rest
    }
}

with <y_true: Tensor>
func cross_entropy(y_pred: Tensor): Tensor {
    return -y_true * log(y_pred)
}

with <model: Model, epochs: int, batch_size: int>
sync func run(input: Tensor) {
    let batches = input.batch(batch_size)
    return wait batches.map(sync (batch: Tensor, index: int) => sync {
        let loss = model(batch)
        if index % 100 == 0 then {
            print('Epoch: ', index, 'Loss: ', loss)
        }
        return loss
    })
}

func main(): int sync {
    let x_train: Tensor = load<npy>('path/to/x_train.npy')
    let y_train: Tensor = load<npy>('path/to/y_train.npy')

    var weights, biases: Tensor[], Tensor[] = [], []
    let mlp = MLP<weights, biases>(784, [(512, relu), (512, relu), (10, softmax)])

    let calc_loss = cross_entropy..reduce_sum
    let forward = mlp..calc_loss<y_train>
    let train = ApplyGradients<opt::adam<lr: 0.1>>(forward)

    weights.fill(randn)
    biases.fill(randn)
    let losses = train.run<10, 128>(x_train)

    losses.plot()

    return 0
}
