module main

import { MLP } from 'mlp'
import { dense, relu, softmax, cross_entropy, Tensor, Model } from 'nn'
import opt from 'opt'
import { fill, run } from 'utils'

func main(): int sync {
    let x_train: Tensor = load<npy>('path/to/x_train.npy')
    let y_train: Tensor = load<npy>('path/to/y_train.npy')

    var weights, biases: Tensor[], Tensor[] = [], []
    let mlp = MLP<weights, biases>(784, [(512, relu), (512, relu), (10, softmax)])

    let calc_loss = cross_entropy..reduce_sum
    let forward = mlp..calc_loss<y_train>
    let train = ApplyGradients<opt::adam<lr: 0.1>>(forward)

    weights.fill(randn)
    biases.fill(randn)
    let losses = train.run<10, 128>(x_train)

    losses.plot()

    return 0
}
